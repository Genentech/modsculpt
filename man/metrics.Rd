% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/metrics.R
\name{metrics}
\alias{metrics}
\alias{score_log_loss}
\alias{score_quadratic}
\alias{check_score_fun}
\alias{metrics_unc}
\alias{metrics_R2}
\alias{metrics_fit_calib}
\alias{metrics_DI}
\alias{metrics_MI}
\alias{metrics_r2}
\title{Various metrics for measuring model performance.}
\usage{
score_log_loss(y, y_hat, na_rm = FALSE)

score_quadratic(y, y_hat, na_rm = FALSE)

check_score_fun(score_fun)

metrics_unc(score_fun, y, na_rm = FALSE)

metrics_R2(score_fun, y, y_hat, na_rm = FALSE)

metrics_fit_calib(y, y_hat, rev_fct = FALSE)

metrics_DI(score_fun, y, y_hat_calib, na_rm = FALSE)

metrics_MI(score_fun, y, y_hat, y_hat_calib, na_rm = FALSE)

metrics_r2(y, y_hat, y_hat_calib, na_rm = FALSE)
}
\arguments{
\item{y}{Vector of observations.}

\item{y_hat}{Vector of predictions.}

\item{na_rm}{Logical, defaults to \code{FALSE}. Should NAs be removed?}

\item{score_fun}{A scoring function: \code{score_quadratic}, \code{score_log_loss},
or a user-defined scoring rule. See below for more details.}

\item{rev_fct}{Logical, defaults to \code{FALSE}. Switch the factor level of
the data before performing calibration. Only relevant for binary response.}

\item{y_hat_calib}{Vector of calibrated predictions. See below for more details.}
}
\value{
\code{metrics_fit_calib} returns an \code{\link[mgcv:gam]{mgcv::gam()}} model fit, otherwise a number.
}
\description{
Various metrics for measuring model performance.
}
\section{Functions}{
\itemize{
\item \code{score_log_loss()}: Binary log loss score

\item \code{score_quadratic()}: Quadratic score

\item \code{check_score_fun()}: Utility function for checking the properties of a user-defined \code{score_fun}.

\item \code{metrics_unc()}: Uncertainty

\item \code{metrics_R2()}: R^2 metric

\item \code{metrics_fit_calib()}: Fit calibration curve using \code{\link[mgcv:gam]{mgcv::gam()}}.
Note that NAs are always dropped.

\item \code{metrics_DI()}: Discrimination index

\item \code{metrics_MI()}: Miscalibration index

\item \code{metrics_r2()}: r^2 metric based on slope of \code{lm}

}}
\section{Scoring function}{

One can use predefined scores like \code{score_quadratic} or \code{score_log_loss}.
If those do not fit the needs, a user-defined scoring function can also be used.
This function needs to take exactly 3 arguments: \code{y} (truth values),
\code{y_hat} (estimated values), and \code{na_rm} (should NAs be removed?):
\itemize{
\item both \code{y} and \code{y_hat} are numeric (not factors!)
\item \code{na_rm} is a scalar logical
}

It needs to return a number.
There is a utility function \code{check_score_fun} to check if the user-defined function is
programmed correctly.
It checks the input and the output, but not if the actual returned value makes sense.
}

\section{Calibration}{

To obtain calibrated predictions,
fit a calibration model and predict based on that model.
Users can use their own calibration model or make use of \code{metrics_fit_calib},
which fits an \code{mgcv::gam()} model with smoother \code{mgcv::s(., k = -1)} (automatic knot selection).
If the input \code{y} is a factor, then a binomial family is used, otherwise a gaussian.
NAs are always dropped.

Continuous response example:

\if{html}{\out{<div class="sourceCode">}}\preformatted{calibration_model <- metrics_fit_calib(
  y = truth,
  y_hat = prediction
)
calib_pred <- predict(calibration_model)
}\if{html}{\out{</div>}}

Binary response example:

\if{html}{\out{<div class="sourceCode">}}\preformatted{calibration_model <- metrics_fit_calib(
  y = factor(truth, levels = c("0", "1")),
  y_hat = prediction
)
calib_pred <- predict(calibration_model, type = "response")
}\if{html}{\out{</div>}}

In the binary case, make sure that:
\itemize{
\item \code{y} is a factor with correct level setting.
Usually "0" is the reference (first) level and "1" is the event (second level).
This may clash with \code{yardstick} setting where
the first level is by default the "event" level.
\item \code{y_hat} are probabilities (not a log of odds).
\item returned calibrated predictions \code{calib_pred} are also probabilities by setting
\code{type = "response"}.
}
}

\examples{
# Scores
score_quadratic(y = c(1.34, 2.8), y_hat = c(1.34, 2.8)) # must be 0
score_quadratic(y = 0.5, 0) # must be 0.5**2 = 0.25

score_log_loss(y = c(0, 1), y_hat = c(0.01, 0.9)) # must be close to 0
score_log_loss(y = 0, y_hat = 0) # undefined

check_score_fun(score_quadratic) # passes without errors

# Metrics based on `lm` model
mod <- lm(hp ~ ., data = mtcars)
truth <- mtcars$hp
pred <- predict(mod)

# calibration fit and calibrated predictions
calib_mod <- metrics_fit_calib(y = truth, y_hat = pred)
calib_pred <- predict(calib_mod)

metrics_unc(score_fun = "score_quadratic", y = truth)
metrics_R2(score_fun = "score_quadratic", y = truth, y_hat = pred)
metrics_DI(score_fun = "score_quadratic", y = truth, y_hat_calib = calib_pred)
metrics_MI(score_fun = "score_quadratic", y = truth, y_hat = pred, y_hat_calib = calib_pred)
# Note that R^2 = DI - MI
metrics_r2(y = truth, y_hat = pred, y_hat_calib = calib_pred)

# Metrics based on `glm` model (logistic regression)
# Note the correct setting of levels
mod <- glm(factor(vs, levels = c("0", "1")) ~ hp + mpg, data = mtcars, family = "binomial")
truth_fct <- factor(mtcars$vs, levels = c("0", "1"))
truth_num <- mtcars$vs
pred <- predict(mod, type = "response") # type = "response" returns probabilities

# calibration fit and calibrated predictions
calib_mod <- metrics_fit_calib(y = truth_fct, y_hat = pred)
calib_pred <- predict(calib_mod, type = "response") # type = "response" returns probabilities

metrics_unc(score_fun = "score_quadratic", y = truth_num)
metrics_R2(score_fun = "score_quadratic", y = truth_num, y_hat = pred)
metrics_DI(score_fun = "score_quadratic", y = truth_num, y_hat_calib = calib_pred)
metrics_MI(score_fun = "score_quadratic", y = truth_num, y_hat = pred, y_hat_calib = calib_pred)
# Note that R^2 = DI - MI
metrics_r2(y = truth_num, y_hat = pred, y_hat_calib = calib_pred)

}
